{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11989135,"sourceType":"datasetVersion","datasetId":7540825}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf_wallets_features_classes_combined = pd.read_csv('/kaggle/input/pretrain1/wallets_features_classes_combined.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#L·ªçc b·ªè timestep v√† c√°c rows tr√πng nhau\ndf_wallets_classification = df_wallets_features_classes_combined\ndf_wallets_classification = df_wallets_classification.drop(columns=['Time step']).drop_duplicates()\ndf_wallets_classification","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# B·ªè unknown (class == 3) - we're classifying only 2 class (1 & 2)\ndata = df_wallets_classification.loc[(df_wallets_classification['class'] != 3), 'address']\ndf_wallets_feature_selected = df_wallets_classification.loc[df_wallets_classification['address'].isin(data)]\ndf_wallets_feature_selected\n# Cuz the actor ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.base import clone \nimport xgboost as xgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Goal: binary classification of 0,1\n# 0: licit, 1: illicit\n\n# change illicit (class-2) to '0' for classification\ny = df_wallets_feature_selected[['class']]\ny = y['class'].apply(lambda x: 0 if x == 2 else 1 ) \n\nX_train, X_test, y_train, y_test = train_test_split(df_wallets_feature_selected,y,test_size=0.30,random_state=15, shuffle=False)\nX_train = X_train.drop(columns=['address', 'class'])\nX_test = X_test.drop(columns=['address', 'class'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = MinMaxScaler()\n\n# Fit scaler ch·ªâ tr√™n X_train\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Apply (transform) l√™n X_test b·∫±ng scaler ƒë√£ fit t·ª´ X_train\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\nX_train_scaled","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ƒê·∫£m b·∫£o y l√† m·∫£ng 1 chi·ªÅu\ny_train = y_train.values.ravel() if hasattr(y_train, 'values') else y_train\ny_test = y_test.values.ravel() if hasattr(y_test, 'values') else y_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train model again","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score\nfrom lightgbm import LGBMClassifier","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model_name, y_true, y_pred):\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred)\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    print(f\"\\n üìå {model_name}\")\n    print(\"Precision: %.3f\" % prec[1])\n    print(\"Recall: %.3f\" % rec[1])\n    print(\"F1 Score: %.3f\" % f1[1])\n    print(\"Micro-Average F1: %.3f\" % micro_f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\ny_pred_rf = rf.predict(X_test_scaled)\nevaluate_model(\"Random Forest\", y_test, y_pred_rf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb.fit(X_train_scaled, y_train)\ny_pred_xgb = xgb.predict(X_test_scaled)\nevaluate_model(\"XGBoost\", y_test, y_pred_xgb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Use best model for feature selection","metadata":{}},{"cell_type":"markdown","source":"## Feature importance","metadata":{}},{"cell_type":"code","source":"#RF & XGB l√† 2 model t·ªët nh·∫•t, XGB c√≥ recall cao h∆°n v√† ensemble RF + XGB > RF => Feature selection s·ª≠ d·ª•ng ensemble\n#feature importance RF & XGB\nrf_importances = rf.feature_importances_\nxgb_importances = xgb.feature_importances_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  DataFrame with RF_Imp & XGB_Imp (Importance of RF & XGB)\nimp_df = pd.DataFrame({\n    'Feature': X_train_scaled.columns if hasattr(X_train_scaled, 'columns') else [f'Feature_{i}' for i in range(X_train_scaled.shape[1])],\n    'RF_Imp': rf_importances,\n    'XGB_Imp': xgb_importances,\n})\n\n# Ensemble - importance count using imp of rf + imp of xgb then divide 2\nimp_df['Imp'] = (imp_df['RF_Imp'] + imp_df['XGB_Imp']) / 2\n\n# Ensemble - importance max\nimp_df['Imp_max'] = imp_df[['RF_Imp', 'XGB_Imp']].max(axis=1)\nimp_df_sorted = imp_df.sort_values(by=\"Imp\", ascending=False)\nimp_df_max_sorted = imp_df.sort_values(by=\"Imp_max\", ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imp_df_sorted.head(20) #imp sorted mean","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imp_df_max_sorted.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## L·∫•y top & bot 20 feature theo Feature importance theo max & mean","metadata":{}},{"cell_type":"code","source":"#FI mean\ntop20_fi = imp_df_sorted.head(20)['Feature'].tolist()\nbottom20_fi = imp_df_sorted.tail(20)['Feature'].tolist()\n\n# FI max\ntop20_fi_max = imp_df_max_sorted.head(20)['Feature'].tolist()\nbottom20_fi_max = imp_df_max_sorted.tail(20)['Feature'].tolist()\n\n# T∆∞∆°ng t·ª± v·ªõi PFI, DCFI sau khi t√≠nh\n# top20_pfi, bottom20_pfi, top20_dcfi, bottom20_dcfi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PFI (Permutation feature importance)","metadata":{}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# 1. T√≠nh PFI ri√™ng cho RF\nperm_rf = PermutationImportance(rf, random_state=42).fit(X_test_scaled, y_test)\npfi_rf = eli5.explain_weights_df(perm_rf, feature_names=X_test_scaled.columns.tolist())\npfi_rf = pfi_rf[['feature', 'weight']].rename(columns={'weight': 'weight_rf'})\n\n# 2. T√≠nh PFI ri√™ng cho XGB\nperm_xgb = PermutationImportance(xgb, random_state=42).fit(X_test_scaled, y_test)\npfi_xgb = eli5.explain_weights_df(perm_xgb, feature_names=X_test_scaled.columns.tolist())\npfi_xgb = pfi_xgb[['feature', 'weight']].rename(columns={'weight': 'weight_xgb'})\n\n# 3. G·ªôp ƒëi·ªÉm PFI t·ª´ 2 model theo feature\npfi_merged = pd.merge(pfi_rf, pfi_xgb, on='feature')\n\n# 4. T√≠nh mean v√† max ƒëi·ªÉm PFI\npfi_merged['weight_mean'] = (pfi_merged['weight_rf'] + pfi_merged['weight_xgb']) / 2\npfi_merged['weight_max'] = pfi_merged[['weight_rf', 'weight_xgb']].max(axis=1)\n\n# 5. S·∫Øp x·∫øp v√† l·∫•y top 20, bottom 20 theo mean\ntop20_pfi_mean = pfi_merged.sort_values('weight_mean', ascending=False).head(20)['feature'].tolist()\nbottom20_pfi_mean = pfi_merged.sort_values('weight_mean', ascending=True).head(20)['feature'].tolist()\n\n# 6. S·∫Øp x·∫øp v√† l·∫•y top 20, bottom 20 theo max\ntop20_pfi_max = pfi_merged.sort_values('weight_max', ascending=False).head(20)['feature'].tolist()\nbottom20_pfi_max = pfi_merged.sort_values('weight_max', ascending=True).head(20)['feature'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DCFI (Drop Column Feature Importance)","metadata":{}},{"cell_type":"code","source":"from sklearn.base import clone\nimport pandas as pd\n\ndef compute_dcfi(model, X_train, y_train, random_state=42):\n    # clone model ƒë·ªÉ tr√°nh ·∫£nh h∆∞·ªüng b√™n ngo√†i\n    model_clone = clone(model)\n    if hasattr(model_clone, 'random_state'):\n        model_clone.random_state = random_state\n    \n    # N·∫øu l√† DataFrame th√¨ l·∫•y .values, n·∫øu l√† numpy th√¨ gi·ªØ nguy√™n\n    X_train_array = X_train.values if hasattr(X_train, 'values') else X_train\n    y_train_array = y_train.values if hasattr(y_train, 'values') else y_train\n    \n    # train tr√™n full features\n    model_clone.fit(X_train_array, y_train_array)\n    benchmark_score = model_clone.score(X_train_array, y_train_array)\n    \n    importances = []\n    for col in range(X_train_array.shape[1]):\n        model_clone = clone(model)\n        if hasattr(model_clone, 'random_state'):\n            model_clone.random_state = random_state\n        \n        # Drop column col: n·∫øu DataFrame th√¨ drop theo t√™n, n·∫øu numpy th√¨ drop theo index\n        if hasattr(X_train, 'columns'):\n            X_subset = X_train.drop(X_train.columns[col], axis=1)\n            X_subset_array = X_subset.values\n        else:\n            X_subset_array = np.delete(X_train_array, col, axis=1)\n        \n        model_clone.fit(X_subset_array, y_train_array)\n        drop_col_score = model_clone.score(X_subset_array, y_train_array)\n        importances.append(benchmark_score - drop_col_score)\n    \n    # L·∫•y t√™n feature n·∫øu c√≥, ho·∫∑c t·∫°o t√™n gi·∫£\n    if hasattr(X_train, 'columns'):\n        features = X_train.columns\n    else:\n        features = [f'Feature_{i}' for i in range(X_train_array.shape[1])]\n    \n    dcfi_df = pd.DataFrame({\n        'Feature': features,\n        'Imp': importances\n    }).sort_values('Imp', ascending=False).reset_index(drop=True)\n    \n    return dcfi_df\n\n\n# T√≠nh DCFI cho RF v√† XGB\ndcfi_rf = compute_dcfi(rf, X_train_scaled, y_train)\ndcfi_xgb = compute_dcfi(xgb, X_train_scaled, y_train)\n\n# G·ªôp v√†o m·ªôt DataFrame chung\ndcfi_df = pd.DataFrame({\n    'Feature': X_train_scaled.columns,\n    'RF_Imp': dcfi_rf['Imp'].values,\n    'XGB_Imp': dcfi_xgb['Imp'].values,\n})\n\n# T·ªïng h·ª£p mean v√† max\ndcfi_df['Imp_mean'] = dcfi_df[['RF_Imp', 'XGB_Imp']].mean(axis=1)\ndcfi_df['Imp_max'] = dcfi_df[['RF_Imp', 'XGB_Imp']].max(axis=1)\n\n# S·∫Øp x·∫øp theo mean ho·∫∑c max t√πy m·ª•c ƒë√≠ch\ndcfi_df_sorted_mean = dcfi_df.sort_values('Imp_mean', ascending=False).reset_index(drop=True)\ndcfi_df_sorted_max = dcfi_df.sort_values('Imp_max', ascending=False).reset_index(drop=True)\n\nprint(\"Top features theo DCFI mean:\")\nprint(dcfi_df_sorted_mean.head(20))\n\nprint(\"\\nTop features theo DCFI max:\")\nprint(dcfi_df_sorted_max.head(20))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-29T02:07:14.583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# 1. FI\nfi_mean_df = imp_df_sorted[['Feature', 'Imp']].rename(columns={'Imp': 'FI_Mean'})\nfi_max_df = imp_df_max_sorted[['Feature', 'Imp']].rename(columns={'Imp': 'FI_Max'})\n\n# 2. PFI\npfi_mean_df = pfi_merged[['feature', 'weight_mean']].rename(columns={'feature': 'Feature', 'weight_mean': 'PFI_Mean'})\npfi_max_df = pfi_merged[['feature', 'weight_max']].rename(columns={'feature': 'Feature', 'weight_max': 'PFI_Max'})\n\n# 3. DCFI\ndcfi_mean_df = dcfi_df[['Feature', 'Imp_mean']].rename(columns={'Imp_mean': 'DCFI_Mean'})\ndcfi_max_df = dcfi_df[['Feature', 'Imp_max']].rename(columns={'Imp_max': 'DCFI_Max'})\n\n# Merge to√†n b·ªô\nmerged = fi_mean_df.merge(pfi_mean_df, on='Feature').merge(dcfi_mean_df, on='Feature')\nmerged_max = fi_max_df.merge(pfi_max_df, on='Feature').merge(dcfi_max_df, on='Feature')\n\n# Chu·∫©n h√≥a\nscaler = MinMaxScaler()\nmerged[['FI_Mean', 'PFI_Mean', 'DCFI_Mean']] = scaler.fit_transform(merged[['FI_Mean', 'PFI_Mean', 'DCFI_Mean']])\nmerged_max[['FI_Max', 'PFI_Max', 'DCFI_Max']] = scaler.fit_transform(merged_max[['FI_Max', 'PFI_Max', 'DCFI_Max']])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean-based t·ªïng h·ª£p\nmerged['Mean_Score'] = merged[['FI_Mean', 'PFI_Mean', 'DCFI_Mean']].mean(axis=1)\nmerged_max['Max_Score'] = merged_max[['FI_Max', 'PFI_Max', 'DCFI_Max']].max(axis=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top v√† bottom 20 theo trung b√¨nh\ntop20_mean = merged.sort_values('Mean_Score', ascending=False).head(20)['Feature'].tolist()\nbottom20_mean = merged.sort_values('Mean_Score', ascending=True).head(20)['Feature'].tolist()\n\n# Top v√† bottom 20 theo max\ntop20_max = merged_max.sort_values('Max_Score', ascending=False).head(20)['Feature'].tolist()\nbottom20_max = merged_max.sort_values('Max_Score', ascending=True).head(20)['Feature'].tolist()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}